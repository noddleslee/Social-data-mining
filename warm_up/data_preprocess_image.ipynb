{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0336c34a",
   "metadata": {},
   "source": [
    "### 1. check data type and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b942660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"../data/\")\n",
    "\n",
    "# tweets.dat \n",
    "tweets_path = data_path / \"tweets.dat\"\n",
    "num_tweets = sum(1 for _ in open(tweets_path, \"r\", encoding=\"utf-8\"))\n",
    "print(f\"=== tweets.dat ===\")\n",
    "print(f\"Total tweets: {num_tweets:,} lines (≈ number of tweets)\")\n",
    "print(\"First 3 lines:\")\n",
    "with open(tweets_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(3):\n",
    "        print(f.readline().strip())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# accounts.tsv\n",
    "accounts_path = data_path / \"accounts.tsv\"\n",
    "accounts_df = pd.read_csv(accounts_path, sep=\"\\t\")\n",
    "print(f\"=== accounts.tsv ===\")\n",
    "print(f\"Total accounts: {len(accounts_df):,} rows\")\n",
    "print(\"Columns:\", list(accounts_df.columns))\n",
    "print(\"First 3 rows:\")\n",
    "display(accounts_df.head(3))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "#  media_list.txt\n",
    "media_list_path = data_path / \"media_list.txt\"\n",
    "num_media = sum(1 for _ in open(media_list_path, \"r\", encoding=\"utf-8\"))\n",
    "print(f\"=== media_list.txt ===\")\n",
    "print(f\"Total media files: {num_media:,} lines (≈ number of images)\")\n",
    "print(\"First 5 lines:\")\n",
    "with open(media_list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f4a61",
   "metadata": {},
   "source": [
    "### 2. Preprocessing\n",
    "Construct an “image-level” table where each row represents one image (media), \n",
    "with its corresponding tweet, author, timestamp, engagement metrics, \n",
    "as well as account metadata (Type/Lang/Stance) and filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f287689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# 1) Read accounts.tsv, disable scientific notation, and treat all columns as strings\n",
    "accounts_path = DATA_DIR / \"accounts.tsv\"\n",
    "accounts_df = pd.read_csv(\n",
    "    accounts_path,\n",
    "    sep=\"\\t\",\n",
    "    dtype=str,            # Prevent author_id from being parsed as scientific notation (e.g., 8.50e+06)\n",
    "    keep_default_na=False  # Prevent empty strings from being converted to NaN\n",
    ")\n",
    "# Standardize column names\n",
    "accounts_df.columns = [c.strip() for c in accounts_df.columns]\n",
    "if \"author_id\" not in accounts_df.columns:\n",
    "    # Try common alternative column names\n",
    "    for alt in [\"user_id\", \"id\", \"account_id\"]:\n",
    "        if alt in accounts_df.columns:\n",
    "            accounts_df = accounts_df.rename(columns={alt: \"author_id\"})\n",
    "            break\n",
    "\n",
    "print(\"=== accounts.tsv (head) ===\")\n",
    "display(accounts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Read media_list.txt, remove file extensions, and create media_list_df = [media_key, file_name]\n",
    "#    Example: \"3_456462992792498176.jpg\" -> media_key = \"3_456462992792498176\"\n",
    "media_list_path = DATA_DIR / \"media_list.txt\"\n",
    "media_rows = []\n",
    "with open(media_list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        fname = line.strip()\n",
    "        if not fname:\n",
    "            continue\n",
    "        stem = Path(fname).stem  # Remove file extension\n",
    "        media_rows.append({\"media_key\": stem, \"file_name\": fname})\n",
    "\n",
    "media_list_df = pd.DataFrame(media_rows, columns=[\"media_key\", \"file_name\"])\n",
    "print(\"\\n=== media_list.txt (head) ===\")\n",
    "display(media_list_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f62989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Read tweets.dat line by line (JSON Lines format)\n",
    "#    Compatible with:\n",
    "#       v2: attachments.media_keys\n",
    "#       v1: entities.media / extended_entities.media (extract id / id_str and reformat as media_key)\n",
    "tweets_path = DATA_DIR / \"tweets.dat\"\n",
    "\n",
    "# The following helper functions (by Professor G) are used to parse tweet JSON,\n",
    "# extract media info, and avoid KeyError or format errors.\n",
    "def safe_get(d, *keys, default=None):\n",
    "    \"\"\"Multi-level get to avoid KeyError.\"\"\"\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def parse_created_at(ts):\n",
    "    if not ts:\n",
    "        return None\n",
    "    # Common format: \"2015-12-12T23:59:59.000Z\"\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_media_entries(tweet):\n",
    "    \"\"\"\n",
    "    Return a list like [ {media_key, source}, ... ]\n",
    "    Priority is given to v2: attachments.media_keys.\n",
    "    If v2 is missing, fallback to v1: entities/extended_entities.media.\n",
    "    These usually contain only id/id_str and type, so we reformat them into\n",
    "    the v2-style media_key = \"3_\" + id_str, so they can match with media_list.txt.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    # --- v2 path: attachments.media_keys ---\n",
    "    media_keys = safe_get(tweet, \"attachments\", \"media_keys\", default=[])\n",
    "    if isinstance(media_keys, list):\n",
    "        for mk in media_keys:\n",
    "            out.append({\n",
    "                \"media_key\": str(mk),\n",
    "                \"source\": \"v2_attachments\"\n",
    "            })\n",
    "\n",
    "    # # --- v1 path: extended_entities.media / entities.media ---\n",
    "    # # If no v2 exists, construct media_key from v1 (Twitter v1 usually has id/id_str only)\n",
    "    # def add_from_media_list(media_list, tag):\n",
    "    #     if isinstance(media_list, list):\n",
    "    #         for m in media_list:\n",
    "    #             mid = str(m.get(\"id_str\") or m.get(\"id\") or \"\").strip()\n",
    "    #             mtype = m.get(\"type\")\n",
    "    #             if mid:\n",
    "    #                 # Typically, v2 media_key looks like \"3_<id>\"; we use the same format for compatibility\n",
    "    #                 mk = f\"3_{mid}\"\n",
    "    #                 out.append({\"media_key\": mk, \"source\": tag})\n",
    "\n",
    "    # ee_media = safe_get(tweet, \"extended_entities\", \"media\", default=None)\n",
    "    # if ee_media:\n",
    "    #     add_from_media_list(ee_media, \"v1_extended_entities\")\n",
    "\n",
    "    # e_media = safe_get(tweet, \"entities\", \"media\", default=None)\n",
    "    # if e_media:\n",
    "    #     add_from_media_list(e_media, \"v1_entities\")\n",
    "\n",
    "    # Deduplicate media entries in the same tweet (by media_key)\n",
    "    unique = {}\n",
    "    for m in out:\n",
    "        unique[m[\"media_key\"]] = m\n",
    "    return list(unique.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7edf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract additional basic fields from each tweet:\n",
    "# tweet_id, author_id, created_at, lang, and engagement metrics \n",
    "# (retweet_count, reply_count, like_count, quote_count)\n",
    "# Each image is stored as one row in image_rows.\n",
    "# Using the image as the base unit: in later aggregation steps (by day or by account),\n",
    "# the unit of analysis will be “number of images” or “engagements received by images”.\n",
    "# Therefore, we split the “tweet:media = 1:n” relationship into one row per image.\n",
    "image_rows = []\n",
    "\n",
    "with open(tweets_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for ln, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            tw = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            # Skip malformed lines\n",
    "            continue\n",
    "\n",
    "        tweet_id = str(tw.get(\"id\", \"\")).strip()\n",
    "        author_id = str(tw.get(\"author_id\", \"\")).strip()\n",
    "        created_at = parse_created_at(tw.get(\"created_at\"))\n",
    "        lang = tw.get(\"lang\")\n",
    "        metrics = tw.get(\"public_metrics\") or {}\n",
    "        retweets = metrics.get(\"retweet_count\")\n",
    "        replies = metrics.get(\"reply_count\")\n",
    "        likes = metrics.get(\"like_count\")\n",
    "        quotes = metrics.get(\"quote_count\")\n",
    "\n",
    "        media_entries = extract_media_entries(tw)\n",
    "        if not media_entries:\n",
    "            continue  # This tweet has no images\n",
    "\n",
    "        for m in media_entries:\n",
    "            image_rows.append({\n",
    "                \"media_key\": m[\"media_key\"],\n",
    "                \"tweet_id\": tweet_id,\n",
    "                \"author_id\": author_id,\n",
    "                \"created_at\": created_at,\n",
    "                \"date\": created_at.date().isoformat() if created_at else None,\n",
    "                \"lang\": lang,\n",
    "                \"retweet_count\": retweets,\n",
    "                \"reply_count\": replies,\n",
    "                \"like_count\": likes,\n",
    "                \"quote_count\": quotes,\n",
    "                \"source_path\": m[\"source\"]  # Record extraction source for quality checking\n",
    "            })\n",
    "\n",
    "images_df = pd.DataFrame(image_rows)\n",
    "\n",
    "print(\"\\n=== Extracted images from tweets (head) ===\")\n",
    "display(images_df.head())\n",
    "print(f\"Total images extracted: {len(images_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Merge with media_list (to get file names) and accounts (to get Type/Lang/Stance)\n",
    "# 4.1 media_key → file_name\n",
    "images_df = images_df.merge(media_list_df, how=\"left\", on=\"media_key\")\n",
    "\n",
    "# 4.2 Account metadata\n",
    "acc_cols = [\"author_id\", \"Type\", \"Lang\", \"Stance\"]  # Keep required columns for the lab: author_id, Type, Lang, Stance\n",
    "for c in acc_cols:\n",
    "    if c not in accounts_df.columns:\n",
    "        # Error tolerance: if any column is missing, create an empty one to prevent merge errors\n",
    "        accounts_df[c] = \"\"\n",
    "# Align at the row level first; after this, we can perform:\n",
    "#   groupby(\"date\") for images_by_day\n",
    "#   groupby(\"author_id\") for images_by_account\n",
    "images_df = images_df.merge(\n",
    "    accounts_df[acc_cols].rename(columns={\"Lang\": \"account_lang\"}),\n",
    "    how=\"left\",\n",
    "    on=\"author_id\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== images_df after merge (head) ===\")\n",
    "display(images_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptive statistics, useful for writing the datasheet\n",
    "print(\"\\n=== Basic stats ===\")\n",
    "# Total number of images (rows)\n",
    "print(\"Total images   :\", len(images_df))\n",
    "# Number of unique media_keys (should equal total images unless duplicates exist)\n",
    "print(\"Unique media_key:\", images_df[\"media_key\"].nunique())\n",
    "# Number of unique authors\n",
    "print(\"Unique authors  :\", images_df[\"author_id\"].nunique())\n",
    "# Date range\n",
    "print(\"Date range      :\", images_df[\"date\"].min(), \"-\", images_df[\"date\"].max())\n",
    "# Top 5 accounts by number of images\n",
    "print(\"\\nTop 5 accounts by image count:\")\n",
    "display(images_df[\"author_id\"].value_counts().head(5).to_frame(\"image_count\"))\n",
    "# Top 5 dates by number of images\n",
    "print(\"\\nTop 5 dates by image count:\")\n",
    "display(images_df[\"date\"].value_counts().head(5).to_frame(\"image_count\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ea627",
   "metadata": {},
   "source": [
    "### 3. Generate two tables for subsequent analysis:\n",
    "##### images_by_day.csv\n",
    "groupby(\"date\") to calculate daily statistics: number of images, total likes/retweets, number of active accounts, etc.\n",
    "##### images_by_account.csv\n",
    "groupby(\"author_id\") to calculate per-account statistics: number of images, engagement summaries, along with Type, Stance, and account_lang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee73e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1) images_by_day.csv\n",
    "images_by_day = (\n",
    "    images_df\n",
    "    .groupby(\"date\", as_index=False)\n",
    "    .agg({\n",
    "        \"media_key\": \"count\",             # Number of images per day\n",
    "        \"author_id\": pd.Series.nunique,   # Number of unique accounts posting images\n",
    "        \"like_count\": \"sum\",              # Total number of likes\n",
    "        \"retweet_count\": \"sum\"            # Total number of retweets\n",
    "    })\n",
    "    .rename(columns={\n",
    "        \"media_key\": \"num_images\",\n",
    "        \"author_id\": \"num_accounts\",\n",
    "        \"like_count\": \"total_likes\",\n",
    "        \"retweet_count\": \"total_retweets\"\n",
    "    })\n",
    "    .sort_values(\"date\")\n",
    ")\n",
    "\n",
    "images_by_day_path = output_dir / \"images_by_day.csv\"\n",
    "images_by_day.to_csv(images_by_day_path, index=False)\n",
    "print(f\"Saved {images_by_day_path} ({len(images_by_day)} rows)\")\n",
    "\n",
    "# Display first few rows\n",
    "display(images_by_day.head())\n",
    "\n",
    "\n",
    "# 2) images_by_account.csv\n",
    "images_by_account = (\n",
    "    images_df\n",
    "    .groupby([\"author_id\", \"Type\", \"Stance\", \"account_lang\"], as_index=False)\n",
    "    .agg({\n",
    "        \"media_key\": \"count\",           # Number of images\n",
    "        \"like_count\": \"sum\",\n",
    "        \"retweet_count\": \"sum\",\n",
    "        \"reply_count\": \"sum\",\n",
    "        \"quote_count\": \"sum\"\n",
    "    })\n",
    "    .rename(columns={\n",
    "        \"media_key\": \"num_images\",\n",
    "        \"like_count\": \"total_likes\",\n",
    "        \"retweet_count\": \"total_retweets\",\n",
    "        \"reply_count\": \"total_replies\",\n",
    "        \"quote_count\": \"total_quotes\"\n",
    "    })\n",
    "    .sort_values(\"num_images\", ascending=False)\n",
    ")\n",
    "\n",
    "images_by_account_path = output_dir / \"images_by_account.csv\"\n",
    "images_by_account.to_csv(images_by_account_path, index=False)\n",
    "print(f\"Saved {images_by_account_path} ({len(images_by_account)} rows)\")\n",
    "\n",
    "# Display first few rows\n",
    "display(images_by_account.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056282d5",
   "metadata": {},
   "source": [
    "### 4. Data Summary（for datasheet）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Total unique images : {images_df['media_key'].nunique():,}\")\n",
    "print(f\"Total accounts      : {images_df['author_id'].nunique():,}\")\n",
    "print(f\"Date range          : {images_df['date'].min()} → {images_df['date'].max()}\")\n",
    "print(f\"Missing file_name   : {images_df['file_name'].isna().sum()}\")\n",
    "\n",
    "# missing ratio by column\n",
    "missing_ratio = images_df.isna().mean().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 columns by missing ratio:\")\n",
    "display(missing_ratio.to_frame(\"missing_ratio\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ae-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
