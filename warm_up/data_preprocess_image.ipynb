{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0336c34a",
   "metadata": {},
   "source": [
    "### 1. check data type and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b942660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"data/\"\n",
    "\n",
    "# tweets.dat\n",
    "print(\"=== tweets.dat ===\")\n",
    "with open(data_path + \"tweets.dat\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline().strip())\n",
    "\n",
    "# accounts.tsv\n",
    "print(\"\\n=== accounts.tsv ===\")\n",
    "accounts_df = pd.read_csv(data_path + \"accounts.tsv\", sep=\"\\t\")\n",
    "print(accounts_df.head())\n",
    "\n",
    "# media_list.txt\n",
    "print(\"\\n=== media_list.txt ===\")\n",
    "with open(data_path + \"media_list.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f4a61",
   "metadata": {},
   "source": [
    "### 2. Preprocessing\n",
    "##### 构造“以图片为粒度”的表，每行是一张图片（media），带上它的 tweet、作者、时间、互动指标、以及账号元数据（Type/Lang/Stance）和文件名。\n",
    "\n",
    "##### 以author_id 做key, 合并图片 - 推文 - 账号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f287689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "\n",
    "# 1) 读取 accounts.tsv，禁用科学计数，全部按字符串处理\n",
    "accounts_path = DATA_DIR / \"accounts.tsv\"\n",
    "accounts_df = pd.read_csv(\n",
    "    accounts_path,\n",
    "    sep=\"\\t\",\n",
    "    dtype=str,           # 防止 author_id 变成 8.50e+06\n",
    "    keep_default_na=False  # 防止空字符串被当成 NaN\n",
    ")\n",
    "# 统一列名\n",
    "accounts_df.columns = [c.strip() for c in accounts_df.columns]\n",
    "if \"author_id\" not in accounts_df.columns:\n",
    "    # 尝试常见备选列名\n",
    "    for alt in [\"user_id\", \"id\", \"account_id\"]:\n",
    "        if alt in accounts_df.columns:\n",
    "            accounts_df = accounts_df.rename(columns={alt: \"author_id\"})\n",
    "            break\n",
    "\n",
    "print(\"=== accounts.tsv (head) ===\")\n",
    "display(accounts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 读 media_list.txt，去掉扩展名，得到 media_key, 存成 media_list_df = [media_key, file_name]\n",
    "#    例如： \"3_456462992792498176.jpg\" -> media_key = \"3_456462992792498176\"\n",
    "media_list_path = DATA_DIR / \"media_list.txt\"\n",
    "media_rows = []\n",
    "with open(media_list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        fname = line.strip()\n",
    "        if not fname:\n",
    "            continue\n",
    "        stem = Path(fname).stem  # 去掉扩展名\n",
    "        media_rows.append({\"media_key\": stem, \"file_name\": fname})\n",
    "\n",
    "media_list_df = pd.DataFrame(media_rows, columns=[\"media_key\", \"file_name\"])\n",
    "print(\"\\n=== media_list.txt (head) ===\")\n",
    "display(media_list_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f62989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 逐行读取 tweets.dat (JSON Lines)\n",
    "#    兼容 v2: attachments.media_keys\n",
    "#         v1: entities.media / extended_entities.media，取 id / id_str 组装成类似 media_key\n",
    "tweets_path = DATA_DIR / \"tweets.dat\"\n",
    "\n",
    "# 以下是G老师写的一些辅助函数, 用于解析推文 JSON, 提取媒体信息, 防止 KeyError 等异常.\n",
    "def safe_get(d, *keys, default=None):\n",
    "    \"\"\"多层 get，避免 KeyError。\"\"\"\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def parse_created_at(ts):\n",
    "    if not ts:\n",
    "        return None\n",
    "    # 常见格式：\"2015-12-12T23:59:59.000Z\"\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_media_entries(tweet):\n",
    "    \"\"\"\n",
    "    返回 [ {media_key, source} , ... ]\n",
    "    优先 v2: attachments.media_keys\n",
    "    如果没有 v2，就去找 v1: entities/extended_entities.media, 这些里通常只有 id/id_str 和 type, 把它拼接成 v2 风格的 media_key = \"3_\" + id_str, 这样就能和 media_list.txt 对上\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    # --- v2 路径：attachments.media_keys ---\n",
    "    media_keys = safe_get(tweet, \"attachments\", \"media_keys\", default=[])\n",
    "    if isinstance(media_keys, list):\n",
    "        for mk in media_keys:\n",
    "            out.append({\n",
    "                \"media_key\": str(mk),\n",
    "                \"source\": \"v2_attachments\"\n",
    "            })\n",
    "\n",
    "    # --- v1 路径：extended_entities.media / entities.media ---\n",
    "    # 如果没有 v2，就尝试从 v1 里构造 media_key（Twitter v1 通常只有 id/id_str）\n",
    "    def add_from_media_list(media_list, tag):\n",
    "        if isinstance(media_list, list):\n",
    "            for m in media_list:\n",
    "                mid = str(m.get(\"id_str\") or m.get(\"id\") or \"\").strip()\n",
    "                mtype = m.get(\"type\")\n",
    "                if mid:\n",
    "                    # 经验上 v2 的 media_key 形如 \"3_<id>\"，这里用同样格式方便对接 media_list\n",
    "                    mk = f\"3_{mid}\"\n",
    "                    out.append({\"media_key\": mk,\"source\": tag})\n",
    "\n",
    "    ee_media = safe_get(tweet, \"extended_entities\", \"media\", default=None)\n",
    "    if ee_media:\n",
    "        add_from_media_list(ee_media, \"v1_extended_entities\")\n",
    "\n",
    "    e_media = safe_get(tweet, \"entities\", \"media\", default=None)\n",
    "    if e_media:\n",
    "        add_from_media_list(e_media, \"v1_entities\")\n",
    "\n",
    "    # 对同一 tweet 里可能重复收集到的媒体去重（用 media_key）\n",
    "    unique = {}\n",
    "    for m in out:\n",
    "        unique[m[\"media_key\"]] = m\n",
    "    return list(unique.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7edf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 tweet 里还取出了一些基本字段：tweet_id, author_id, created_at, lang, 以及互动指标（retweet_count, reply_count, like_count, quote_count）\n",
    "# 每张图片一行记录到 image_rows\n",
    "# 以图片为粒度：后续按天、按账号聚合时，单位是“图片数”“图片收到的互动数”。所以一开始就把“tweet:media = 1:n”拆成每图一行。\n",
    "image_rows = []\n",
    "\n",
    "with open(tweets_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for ln, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            tw = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            # 有脏行时跳过\n",
    "            continue\n",
    "\n",
    "        tweet_id = str(tw.get(\"id\", \"\")).strip()\n",
    "        author_id = str(tw.get(\"author_id\", \"\")).strip()\n",
    "        created_at = parse_created_at(tw.get(\"created_at\"))\n",
    "        lang = tw.get(\"lang\")\n",
    "        metrics = tw.get(\"public_metrics\") or {}\n",
    "        retweets = metrics.get(\"retweet_count\")\n",
    "        replies = metrics.get(\"reply_count\")\n",
    "        likes = metrics.get(\"like_count\")\n",
    "        quotes = metrics.get(\"quote_count\")\n",
    "\n",
    "        media_entries = extract_media_entries(tw)\n",
    "        if not media_entries:\n",
    "            continue  # 本推文没有图片\n",
    "\n",
    "        for m in media_entries:\n",
    "            image_rows.append({\n",
    "                \"media_key\": m[\"media_key\"],\n",
    "                \"tweet_id\": tweet_id,\n",
    "                \"author_id\": author_id,\n",
    "                \"created_at\": created_at,\n",
    "                \"date\": created_at.date().isoformat() if created_at else None,\n",
    "                \"lang\": lang,\n",
    "                \"retweet_count\": retweets,\n",
    "                \"reply_count\": replies,\n",
    "                \"like_count\": likes,\n",
    "                \"quote_count\": quotes,\n",
    "                \"source_path\": m[\"source\"]  # 记录提取来源，便于质量检查\n",
    "            })\n",
    "\n",
    "images_df = pd.DataFrame(image_rows)\n",
    "\n",
    "print(\"\\n=== Extracted images from tweets (head) ===\")\n",
    "display(images_df.head())\n",
    "print(f\"Total images extracted: {len(images_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) 关联 media_list（拿到文件名）、accounts（拿到 Type/Lang/Stance）\n",
    "# 4.1 media_key → file_name\n",
    "images_df = images_df.merge(media_list_df, how=\"left\", on=\"media_key\")\n",
    "\n",
    "# 4.2 账号元数据\n",
    "acc_cols = [\"author_id\", \"Type\", \"Lang\", \"Stance\"] # 保留lab里要求的列：author_id, Type, Lang, Stance\n",
    "for c in acc_cols:\n",
    "    if c not in accounts_df.columns:\n",
    "        # 容错：如果没这些列，就用空列占位，避免 merge 报错\n",
    "        accounts_df[c] = \"\"\n",
    "# 先在行级别对齐，之后就可以：\n",
    "#   groupby(\"date\") 做 images_by_day\n",
    "#   groupby(\"author_id\") 做 images_by_account\n",
    "images_df = images_df.merge(\n",
    "    accounts_df[acc_cols].rename(columns={\"Lang\": \"account_lang\"}),\n",
    "    how=\"left\",\n",
    "    on=\"author_id\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== images_df after merge (head) ===\")\n",
    "display(images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础描述性统计，便于写 datasheet\n",
    "print(\"\\n=== Basic stats ===\")\n",
    "# 总图片数（行数）\n",
    "print(\"Total images   :\", len(images_df))\n",
    "# 不同 media_key 数（理论上和总图片数相等，除非有重复记录）\n",
    "print(\"Unique media_key:\", images_df[\"media_key\"].nunique())\n",
    "# 不同作者数\n",
    "print(\"Unique authors  :\", images_df[\"author_id\"].nunique())\n",
    "# 时间范围\n",
    "print(\"Date range      :\", images_df[\"date\"].min(), \"-\", images_df[\"date\"].max())\n",
    "# 统计发图最多的 5 个账号\n",
    "print(\"\\nTop 5 accounts by image count:\")\n",
    "display(images_df[\"author_id\"].value_counts().head(5).to_frame(\"image_count\"))\n",
    "# 发图最多的 5 天\n",
    "print(\"\\nTop 5 dates by image count:\")\n",
    "display(images_df[\"date\"].value_counts().head(5).to_frame(\"image_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ea627",
   "metadata": {},
   "source": [
    "### 3. 生成后续分析需要的两张表:\n",
    "##### images_by_day.csv\n",
    "groupby(\"date\") 统计每一天：图片数量、总 likes/retweets、发图账号数等。\n",
    "##### images_by_account.csv\n",
    "groupby(\"author_id\") 统计每个账号：图片数量、互动汇总，附带 Type/stance/account_lang。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee73e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1) images_by_day.csv\n",
    "images_by_day = (\n",
    "    images_df\n",
    "    .groupby(\"date\", as_index=False)\n",
    "    .agg({\n",
    "        \"media_key\": \"count\",             # 每天的图片数量\n",
    "        \"author_id\": pd.Series.nunique,   # 发图账号数\n",
    "        \"like_count\": \"sum\",              # 总点赞数\n",
    "        \"retweet_count\": \"sum\"            # 总转推数\n",
    "    })\n",
    "    .rename(columns={\n",
    "        \"media_key\": \"num_images\",\n",
    "        \"author_id\": \"num_accounts\",\n",
    "        \"like_count\": \"total_likes\",\n",
    "        \"retweet_count\": \"total_retweets\"\n",
    "    })\n",
    "    .sort_values(\"date\")\n",
    ")\n",
    "\n",
    "images_by_day_path = output_dir / \"images_by_day.csv\"\n",
    "images_by_day.to_csv(images_by_day_path, index=False)\n",
    "print(f\"Saved {images_by_day_path} ({len(images_by_day)} rows)\")\n",
    "\n",
    "# 打印前几行\n",
    "display(images_by_day.head())\n",
    "\n",
    "\n",
    "# 2)images_by_account.csv\n",
    "images_by_account = (\n",
    "    images_df\n",
    "    .groupby([\"author_id\", \"Type\", \"Stance\", \"account_lang\"], as_index=False)\n",
    "    .agg({\n",
    "        \"media_key\": \"count\",           # 图片数\n",
    "        \"like_count\": \"sum\",\n",
    "        \"retweet_count\": \"sum\",\n",
    "        \"reply_count\": \"sum\",\n",
    "        \"quote_count\": \"sum\"\n",
    "    })\n",
    "    .rename(columns={\n",
    "        \"media_key\": \"num_images\",\n",
    "        \"like_count\": \"total_likes\",\n",
    "        \"retweet_count\": \"total_retweets\",\n",
    "        \"reply_count\": \"total_replies\",\n",
    "        \"quote_count\": \"total_quotes\"\n",
    "    })\n",
    "    .sort_values(\"num_images\", ascending=False)\n",
    ")\n",
    "\n",
    "images_by_account_path = output_dir / \"images_by_account.csv\"\n",
    "images_by_account.to_csv(images_by_account_path, index=False)\n",
    "print(f\"Saved {images_by_account_path} ({len(images_by_account)} rows)\")\n",
    "\n",
    "# 打印前几行\n",
    "display(images_by_account.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056282d5",
   "metadata": {},
   "source": [
    "### 4. 数据质量简报（用于 datasheet）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Total unique images : {images_df['media_key'].nunique():,}\")\n",
    "print(f\"Total accounts      : {images_df['author_id'].nunique():,}\")\n",
    "print(f\"Date range          : {images_df['date'].min()} → {images_df['date'].max()}\")\n",
    "print(f\"Missing file_name   : {images_df['file_name'].isna().sum()}\")\n",
    "\n",
    "# 检查空值比例\n",
    "missing_ratio = images_df.isna().mean().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 columns by missing ratio:\")\n",
    "display(missing_ratio.to_frame(\"missing_ratio\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ae-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
